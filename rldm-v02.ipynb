{
 "metadata": {
  "name": "",
  "signature": "sha256:8e2d951df4795699b18ed2b241b41736f412818a2dcc9fd3d0c74ed763266e64"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "#<center>Application de l'apprentissage par renforcement au processus de d\u00e9cisions s\u00e9quentielles.</center>\n",
      "\n",
      "      \n",
      "#####Version 0.1\n",
      "#####Adel Cha\u00efbi : 5 Novembre 2014\n",
      "\n",
      " <br\\>\n",
      " <br\\>\n",
      "## 1. Introduction\n",
      "      \n",
      "La prise de d\u00e9cision est un processus cognitif complexe visant \u00e0 la s\u00e9lection d'un type d'action parmi diff\u00e9rentes alternatives. Chaque processus de la prise de d\u00e9cision produit un choix final. Le r\u00e9sultat peut \u00eatre une action ou une opinion d'un choix. (Wikip\u00e9dia).\n",
      "      \n",
      "Ce processus de d\u00e9cision s'applique \u00e9galement aux entreprises. Chaque d\u00e9cision provoque une action qui g\u00e9n\u00e8re un r\u00e9sultat positif ou n\u00e9gatif. G\u00e9n\u00e9ralement ces d\u00e9cisions sont prises de mani\u00e8re isol\u00e9e, sans prendre en compte l'aspect s\u00e9quentiel et les interactions qui peuvent y avoir entre ces diff\u00e9rentes d\u00e9cisions.\n",
      "      \n",
      "\n",
      "\n",
      "Prenons l'exemple du ciblage client sur une campagne de promotion. En se basant sur l'historique des promotions, nous pouvons imaginer une classification qui nous donnerait les clients susceptibles de r\u00e9pondre positivement \u00e0 chaque campagne. Donc nous retenons uniquement les clients qui maximiseraient le gain imm\u00e9diat et ceci sur chaque promotion. Or cette approche n'est pas forcement rentable \u00e0 plus long terme. En effet Il se peut qu'une action au pr\u00e8s d'un client donn\u00e9, ne soit pas rentable dans l'imm\u00e9diat mais elle peut \u00e9ventuellement renforcer sa fid\u00e9lit\u00e9 et \u00e9largir son panier dans le future. A l'inverse, une fr\u00e9quence \u00e9lev\u00e9e de promotions envers certains consommateurs peut avoir l'effet inverse, et ainsi faire baisser le profit g\u00e9n\u00e9r\u00e9 par ces consommateurs. (Spam, budget limit\u00e9,....)\n",
      "      \n",
      "      \n",
      "Afin d'adresser ce genre de probl\u00e9matique, nous utiliserons L'apprentissage par renforcement (RL).\n",
      "      \n",
      "<br\\>\n",
      "\n",
      "\n",
      "\n",
      "## 2. Apprentissage par renforcement\n",
      "      \n",
      "      \n",
      "L'apprentissage par renforcement <b>(RL)</b> fait r\u00e9f\u00e9rence \u00e0 une classe de probl\u00e8mes d'apprentissage automatique, dont le but est d'apprendre, \u00e0 partir d'exp\u00e9riences, ce qu'il convient de faire en diff\u00e9rentes situations, de fa\u00e7on \u00e0 optimiser une r\u00e9compense quantitative au cours du temps. \n",
      "      \n",
      "Un paradigme classique pour pr\u00e9senter les probl\u00e8mes d'apprentissage par renforcement consiste \u00e0 consid\u00e9rer un agent autonome, plong\u00e9 au sein d'un environnement, et qui doit prendre des d\u00e9cisions en fonction de son \u00e9tat courant. En retour, l'environnement procure \u00e0 l'agent une r\u00e9compense, qui peut \u00eatre positive ou n\u00e9gative.\n",
      "      \n",
      "L'agent cherche, \u00e0 travers des exp\u00e9riences it\u00e9r\u00e9es, un comportement d\u00e9cisionnel (appel\u00e9 strat\u00e9gie ou politique, et qui est une fonction associant \u00e0 l'\u00e9tat courant l'action \u00e0 ex\u00e9cuter) optimal, c'est en ce sens qu'il maximise la somme des r\u00e9compenses au cours du temps. (Wikip\u00e9dia)\n",
      "      \n",
      "\n",
      "En particulier, on se basera sur les [MDP (Processus de d\u00e9cision markovien) ](http://fr.wikipedia.org/wiki/Processus_de_d%C3%A9cision_markovien), qui traitent des cas discrets. En effet l'exemple du ciblage Marketing est une suite d'\u00e9v\u00e8nements qui se r\u00e9alisent \u00e0 des instants $t = 0, 1, 2, 3...$\n",
      "     \n",
      "\n",
      "### Terminologie\n",
      "Dans l'apprentissage par Renforcement :\n",
      "\n",
      "* L'agent interagit avec l'environnement. Il est \u00e0 la fois l'apprenant et le d\u00e9cideur.\n",
      "* L'environnement est tout ce qui est ext\u00e9rieur \u00e0 l'agent. \n",
      "* L'agent et l'environnement interagissent de fa\u00e7on continue (Horizon infini, condition importante pour l'application des MDP)\n",
      "* L'agent effectue des actions et l'environnement change d'\u00e9tat et retourne une r\u00e9compense \u00e0 l'agent.\n",
      "* L'agent apprend \u00e0 choisir les actions qui maximiseraient ces gains.\n",
      "* R\u00e9compense imm\u00e9diate vs R\u00e9compense cumul\u00e9e (Future).\n",
      "\n",
      "Dans le cas du Ciblage Marketing, l'agent serait le Retailler et l'environnement le client.\n",
      "\n",
      "<br\\>\n",
      "### 2.1 D\u00e9finition Math\u00e9matique du Processus de D\u00e9cision Markovien\n",
      "\n",
      "Un MDP est d\u00e9fini par :    \n",
      "\n",
      "\n",
      "\n",
      "- States (Etats) : $S$  => L'ensemble des \u00e9tats possibles du syst\u00e8me qui subit l'action.\n",
      "\n",
      "- Actions : A(S), A => L'ensemble des actions que peut entreprendre l'agent afin d'agir sur le syst\u00e8me.\n",
      "      \n",
      "- Model : $T(s,a,s')$ ~ $P(s'/s,a)$ => Fonction probabiliste qui donne la probabilit\u00e9 que le syst\u00e8me passe \u00e0 l'\u00e9tat $s'$ sachant qu'il est \u00e0 l'\u00e9tat $s$ et qu'il subit l'action $a$. \n",
      "      \n",
      "- Reward (R\u00e9compense) : R(s), R(s,a), R(s,a,s') => La r\u00e9compense que re\u00e7oit l'agent en entreprenant l'action $a$ sur le syst\u00e8me. G\u00e9n\u00e9ralement not\u00e9e $r$. \n",
      "            \n",
      "- Policy (Politique) : $\\pi(s)$ -> $a$ => Fonction qui mappe un \u00e9tat donn\u00e9 $s$ \u00e0 une action $a$.        \n",
      "\n",
      "Ainsi, lorsque l'agent entreprend l'action $a$, il re\u00e7oit une r\u00e9compense imm\u00e9diate $R(s)=r$, et provoque la transition de l'environnement de l'\u00e9tat $s$ \u00e0 l'\u00e9tat $s'$ avec une probabilit\u00e9 $P(s'/s,a)$.\n",
      "\n",
      "G\u00e9n\u00e9ralement, la probabilit\u00e9 que l'environnement soit \u00e0 l'\u00e9tat $s'$ \u00e0 l'instant $t+1$ d\u00e9pend de tous ces pr\u00e9c\u00e9dents \u00e9tats, autrement dit :\n",
      "\n",
      "\n",
      "$$P(s_{t+1}=s'|s_t, a_t, s_{t-1}, a_{t-1}, ...,s_0, a_0)$$\n",
      "\n",
      "Si par contre, la probabilit\u00e9 que l'environnement soit \u00e0 l'\u00e9tat $s'$ \u00e0 l'instant $t+1$ d\u00e9pend uniquement de l'\u00e9tat et l'action \u00e0 l'instant $t$, alors on pourra dire que la propri\u00e9t\u00e9 de Markov est v\u00e9rifi\u00e9e. Et nous pouvons \u00e9crire :\n",
      "\n",
      "$$P(s_{t+1}=s'/s_t,a_t)$$\n",
      "\n",
      "      \n",
      "<br\\>\n",
      "### 2.2 Application du MDP au cas du \"Ciblage Marketing\"\n",
      "\n",
      "Bas\u00e9 sur l'historique de consommation des promotions, un client donn\u00e9 se trouve dans \u00e9tat $s$. Le Retailler effectue une action $a$ sur ce client. Cette action provoque la transition du client de l'\u00e9tat $s$ \u00e0 l'\u00e9tat $s'$ avec une probabilit\u00e9 $P(s'/s,a)$ et une r\u00e9compense (reward) potentielle $R(s)$ (Profit imm\u00e9diat). Ce processus continue dans le temps tant qu'il y'a une relation (Action/R\u00e9action) entre le consommateur et le retailler. \n",
      "$R(S)$ repr\u00e9sente le net \u00e0 gagner, en prenant en compte les d\u00e9penses du consommateur et les coups de l'action ().\n",
      "\n",
      "Formellement, cela revient \u00e0 d\u00e9finir {S, A, T, R} comme suit :\n",
      "\n",
      "\n",
      "- States (Etats)  $S$ : Les \u00e9tats possibles du client. S peut \u00eatre repr\u00e9sent\u00e9 par un vecteur contenant toutes les donn\u00e9es connues sur le client \u00e0 l'instant t.\n",
      "\n",
      "- Actions A(S), A : Ensemble fini des actions que peut entreprendre l'agent, ici le Retailler.\n",
      "\n",
      "- Model, $T(s,a,s')$ ~ $P(s'/s,a)$ : Fonction de transition qui repr\u00e9sente la probabilit\u00e9 que le client passe \u00e0 l'\u00e9tat $s'$, sachant qu'il est \u00e0 l'\u00e9tat $s$ et qu'il subit l'action $a$.\n",
      "\n",
      "- Reward (R\u00e9compense), R(s), R(s,a), R(s,a,s') : Le net \u00e0 gagner pour le Retailler apr\u00e8s avoir agi (actions $a$) sur le client.\n",
      "\n",
      "\n",
      "Ce que nous cherchons \u00e0 d\u00e9finir, c'est la politique $\\pi$. Cette fonction permet au Retailler de savoir quelle est l'action la plus rentable sachant que le client est \u00e0 l'\u00e9tat $s$. Math\u00e9matiquement parlant, cela revient \u00e0 mapper les \u00e9tats aux actions :\n",
      "$\\pi(s)=a$. Mais  il y a une condition n\u00e9cessaire qui nous permet d'\u00e9crire cela, nous consid\u00e9rons le temps comme infini. Autrement, il faudrait consid\u00e9rer le temps comme un param\u00e8tre de la fonction $\\pi$ :  $\\pi(s,t)$ -> $a$. Comme pr\u00e9cis\u00e9 plus haut, l'int\u00e9r\u00eat de l'agent (ici le Retailler), est de maximiser ses r\u00e9compenses futures attendues (R\u00e9compenses cumul\u00e9es) $R$.\n",
      "            \n",
      "$$R = \\sum_{t=0}^{\\infty}R(s_t)$$ \n",
      "\n",
      "Sachant que $R(s_t)=r_{t}$. \n",
      "\n",
      "Or cette somme tend vers l'infini quand $t {\\longrightarrow} {\\infty}$. Comme nous consid\u00e9rons que le temps est infini, nous pouvons \u00e9crire :\n",
      "      \n",
      "$$ R = \\sum_{t=0}^{\\infty}R(s_t) = \\sum_{t=0}^{\\infty}\\gamma^{t}R(s_t) = \\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}$$   avec $$0 < \\gamma < 1 $$.\n",
      "      \n",
      "Nous avons tout simplement pond\u00e9r\u00e9 la somme pr\u00e9c\u00e9dente par le coefficient $\\gamma^{t}$, dans un horizon infini, les deux sommes se valent, seulement la deuxi\u00e8me somme va cro\u00eetre moins rapidement que la premi\u00e8re. Posons $r_{max}$, la valeur maximale que peut atteindre $r_{t}$, nous pouvons dire que :\n",
      "      \n",
      "$$ \\sum_{t=0}^{\\infty}\\gamma^{t}r_{t} \\leq \\sum_{t=0}^{\\infty}\\gamma^{t}r_{max}$$\n",
      "      \n",
      "La somme $\\sum_{t=0}^{\\infty}\\gamma^{t}r_{max}$ repr\u00e9sente une s\u00e9rie g\u00e9om\u00e9trique dont la valeur est $\\frac{r_{max}}{1-\\gamma}$, nous obtenons alors :\n",
      "      \n",
      "$$ \\sum_{t=0}^{\\infty}\\gamma^{t}r_{t} \\leq \\frac{r_{max}}{1-\\gamma} $$\n",
      "      \n",
      "$r_{max}$ \u00e9tant une constante, donc $\\frac{r_{max}}{1-\\gamma}$ l'est aussi. Nous avons r\u00e9ussi \u00e0 borner la somme $\\sum_{t=0}^{\\infty}\\gamma^{t}r_{t}$. M\u00eame si cette somme continue \u00e0 cro\u00eetre dans le temps, il est possible de calculer sa valeur. Nous pouvons ajouter une infinit\u00e9 de nombres mais nous obtenons tout de m\u00eame un nombre fini. \n",
      "A noter que la pond\u00e9ration par $\\gamma^{t}r_{t}$ permet de discriminer les r\u00e9compenses les plus avanc\u00e9es dans le futur si on peut dire. Ainsi une r\u00e9compense d'un futur proche aura plus de poids qu'une r\u00e9compense dans un futur lointain, puisque pr\u00e9dire une valeur future assez proche est plus pertinent. \n",
      "      \n",
      "<br\\>\n",
      "\n",
      "### 2.3 Fonction de Valeur et Equation de Bellman\n",
      "Nous cherchons une politique $\\pi$ qui maximiserait les r\u00e9compenses cumul\u00e9es d\u00e9finies par l'\u00e9quation (1). Supposons qu'une telle politique existe, nous la noterons $\\pi^*$. Pour cela on se basera sur la fonction de valeur. La fonction de valeur permet d'estimer le potentiel d'un \u00e9tat ou le potentiel d'effectuer une action dans un \u00e9tat suivant une politique $\\pi$. Autrement dit, c'est la r\u00e9compense future esp\u00e9r\u00e9e par l'agent en partant de l'\u00e9tat $s$ et en  suivant la politique $\\pi$\n",
      "\n",
      "\n",
      "\n",
      "\\begin{equation}\n",
      "V^\\pi(s) = E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^tr_t | \\pi, s_0 = s_t]       ........................................ (1)\n",
      "\\end{equation}\n",
      "\n",
      "\n",
      "\n",
      "De la m\u00eame mani\u00e8re, $Q^\\pi(s,a)$ d\u00e9finit la valeur esp\u00e9r\u00e9e par l'agent en partant de l'\u00e9tat $s$ via l'action $a$ et en suivant la politique $\\pi$.\n",
      "\n",
      "\\begin{align}\n",
      "Q^\\pi(s,a) &= E_{\\pi}[\\sum_{t=0}^{\\infty}\\gamma^tr_t | s_0 = s_t, a_0 = a_t ] \\\\\n",
      "           &= E_{\\pi}[r_{t+1} + \\sum_{t=1}^{\\infty}\\gamma^tr_t | s_0 = s_{t}, a_0 = a_{t}]\\\\\n",
      "           &= E_{\\pi}[r_{t+1} + \\gamma V^{\\pi}(s_{t+1}) | s_0 = s_{t}, a_0 = a_{t}] .......................... (2)\\\\\n",
      "\\end{align}\n",
      "\n",
      "\n",
      "La d\u00e9monstration de l'\u00e9quation (2) est triviale. C'est uniquement \u00e0 l'instant $t$ en partant de l'\u00e9tat $s$ ou l'action \u00e0 effectuer est fig\u00e9e, \u00e0 partir de l'\u00e9tat suivant $s_{t+1}$, nous nous retrouvons dans la m\u00eame situation que l'\u00e9quation (1). A La politique optimale $\\pi^*$, correspond la fonction de valeur d'action $Q^*(s,a)$, dont elle d\u00e9rive : $\\pi^{*}(s) = \\operatorname{arg\\,max}_a Q^{*}(s,a)$. Or\n",
      "\n",
      "\\begin{align}\n",
      "Q^*(s,a) = \\operatorname{max}_a Q^{\\pi}(s,a)\n",
      "\\end{align}\n",
      "\n",
      "Ce qui nous donne :\n",
      "\n",
      "\\begin{align} \n",
      "Q^*(s,a) = E_{\\pi}[r_{t+1} + \\gamma V^*(s_{t+1}) | s_0 = s_{t}, a_0 = a_{t}]\n",
      "\\end{align}\n",
      "\n",
      "Et nous avons\n",
      "\n",
      "\\begin{align} \n",
      "V^*(s) = \\operatorname{max_a} Q^*(s, a)\n",
      "\\end{align}\n",
      "\n",
      "Nous obtenons alors \n",
      "\n",
      "\\begin{align} \n",
      "Q^*(s,a) = E_{\\pi^*}[r_{t+1} + \\gamma \\operatorname{max}_{a'}Q^*(s_{t+1}, a') | s_0 = s_{t}, a_0 = a_{t}]\n",
      "\\end{align}\n",
      "\n",
      "\n",
      "\n",
      "C'est la fameuse \u00e9quation de Bellman, qui permet de d\u00e9finir la politique optimale $\\pi^*$. La premi\u00e8re partie de l'\u00e9quation $E_{r}[r|s,a]$ repr\u00e9sente la r\u00e9compense imm\u00e9diate g\u00e9n\u00e9r\u00e9e par l'action $a$ qui provoque la transition de l'\u00e9tat $s$ \u00e0 l'\u00e9tat $s'$. La deuxi\u00e8me partie de l'\u00e9quation repr\u00e9sente la r\u00e9compense future esp\u00e9r\u00e9e par l'agent \u00e0 partir de l'\u00e9tat $s'$. L'\u00e9quation de Bellman peut \u00eatre r\u00e9solue de mani\u00e8re r\u00e9cursive :\n",
      "\n",
      "\\begin{align}\n",
      "Q_0(s, a) &= R(s, a)\\\\\n",
      "Q_{k+1}(s,a) &= R(s,a) + \\gamma\\sum_{s'}P(s'|s,a)\\operatorname{max}_{a'}Q_k(s',a')\n",
      "\\end{align}\n",
      "\n",
      " \n",
      "Dans la plus part des cas d'applications, la r\u00e9compense imm\u00e9diate $R(s,a)$ et le mod\u00e8le $P(s'|s,a)$ sont des inconnus. Il devient ainsi difficile d'induire la fonction de valeur optimale. D'autres approches par simulation permettent d'approximer $Q^*(s,a)$. Parmi lesquelles, la m\u00e9thode de Q-learning.\n",
      "\n",
      "<br\\>\n",
      "### 2.4  Q-learning\n",
      "\n",
      " * $\\textbf{Remarque}$ : Ces deux expressions sont \u00e9quivalentes.\n",
      "\n",
      "$$v \\xleftarrow{\\alpha}x \\Leftrightarrow v \\leftarrow (1 - \\alpha)v + \\alpha x$$\n",
      "\n",
      "Prenons l'exemple suivant :\n",
      "\n",
      "Soit :\n",
      "\n",
      "* $\\alpha_{t}$ fonction de $t$ tel que  \n",
      "\n",
      "\\begin{cases}\\sum_{t=1}^{\\infty}\\alpha_t = \\infty \\\\\n",
      "\\sum_{t=1}^{\\infty}\\alpha_t^2 < \\infty \\\\\n",
      "Exemple : \\alpha_t = \\frac{1}{t}\n",
      "\\end{cases}\n",
      "\n",
      "\n",
      "* $X_t, X_{t+1}, X_{t+2},... \\in X$ un ensemble de variables al\u00e9atoires ind\u00e9pendantes et identiquement distribu\u00e9es (iid).\n",
      "\n",
      "\n",
      "* $V_t \\xleftarrow{\\alpha_t}X_t $\n",
      "\n",
      "\n",
      "Alors \n",
      "\n",
      "* $V_t$ => $E[X_t]$\n",
      "\n",
      "Cette m\u00e9thode est appel\u00e9e la m\u00e9thode Monte Carlos. La m\u00e9thode Monte Carlos est une des m\u00e9thodes de simulation bas\u00e9es sur le tirage de nombres au hasard. Nous utiliserons la m\u00eame m\u00e9thode pour calculer $Q$. En partant d'une valeur initiale al\u00e9atoire $\\hat{Q}$, puis en it\u00e9rant jusqu'\u00e0 convergence de $\\hat{Q}$ vers $Q$.\n",
      "\n",
      "\\begin{align}\n",
      " \\hat{Q}(s,a) &\\xleftarrow{\\alpha_t} r + \\gamma\\operatorname{max_a}\\hat{Q}(s',a') \\\\\n",
      "        &= E[r + \\gamma\\operatorname{max_a}\\hat{Q}(s',a') ] \\\\\n",
      "        &= R(s) + E[\\gamma\\operatorname{max_a}\\hat{Q}(s',a') ] \\\\\n",
      "        &= R(s) + \\gamma\\sum T(s,a,s')\\operatorname{max_a}\\hat{Q}(s',a')  \\\\\n",
      "\\end{align}\n",
      "\n",
      "Ainsi, nous arrivons \u00e0 estimer la valeur optimale de $Q$ par simulation, et ceci malgr\u00e9 que l\u2019on n\u2019ait pas le mod\u00e8le.\n",
      "\n",
      "<br\\>\n",
      "## Conclusion\n",
      "\n",
      "Comme nous l'avons vu, il est possible gr\u00e2ce aux Algorithmes d'apprentissage par Renforcement de construire des mod\u00e8les pr\u00e9dictive en prenant en compte l'aspect s\u00e9quentiel.\n",
      "Dans cet article, nous avons pris comme exemple d'application le ciblage marketing, mais cette m\u00e9thode s'applique \u00e0 tout processus s\u00e9quentiel <b>[4]</b>. Un point important concernant l'utilisation du RL est la scalabilit\u00e9. Que ce soit d'un point de vue de volum\u00e9trie des donn\u00e9es mais \u00e9galement la nature it\u00e9rative de l'apprentissage par renforcement qui n\u00e9cessite la g\u00e9n\u00e9ration d'une s\u00e9quence de mod\u00e8les \u00e0 partir de ces donn\u00e9es. Ces algorithmes s\u2019adaptent donc parfaitement avec le processing sur Cluster. Un dernier point, nous avons consid\u00e9r\u00e9 dans notre cas d'\u00e9tude le monde comme \u00e9tant mono-agent, ce qui est rarement le cas. Une suite possible de ces m\u00e9thodes serait de traiter le cas Multi-agents. En effet, dans le Ciblage Marketing, un  retailler donn\u00e9e n'est pas le seul \u00e0 agir sur le client.\n",
      "<br\\>\n",
      "### R\u00e9f\u00e9rences\n",
      "\n",
      "-  [1] Sutton & Barto: [Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning):](http://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262193981)\n",
      "-  [2]  [Bellman equation](http://en.wikipedia.org/wiki/Bellman_equation)\n",
      "-  [3] [Q-Learning](http://www.acm.uiuc.edu/sigart/docs/QLearning.pdf)\n",
      "-  [4] Raju, C.V.L.  : Reinforcement learning applications in dynamic pricing of retail markets, E-Commerce, 2003. CEC 2003. IEEE International Conference on \n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "\n",
      "\n",
      "def css_styling():\n",
      "    styles = open(\"./styles/custom.css\", \"r\").read()\n",
      "    return HTML(styles)\n",
      "css_styling()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<style>\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-weight: bold;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-style: oblique;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
        "    }\n",
        "    @font-face {\n",
        "        font-family: \"Computer Modern\";\n",
        "        font-weight: bold;\n",
        "        font-style: oblique;\n",
        "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
        "    }\n",
        "    div.cell{\n",
        "        width:800px;\n",
        "        margin-left:16% !important;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    h1 {\n",
        "        font-family: Helvetica, serif;\n",
        "    }\n",
        "    h4{\n",
        "        margin-top:12px;\n",
        "        margin-bottom: 3px;\n",
        "       }\n",
        "    div.text_cell_render{\n",
        "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
        "        line-height: 145%;\n",
        "        font-size: 130%;\n",
        "        width:800px;\n",
        "        margin-left:auto;\n",
        "        margin-right:auto;\n",
        "    }\n",
        "    .CodeMirror{\n",
        "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
        "    }\n",
        "    .prompt{\n",
        "        display: None;\n",
        "    }\n",
        "    .text_cell_render h5 {\n",
        "        font-weight: 300;\n",
        "        font-size: 22pt;\n",
        "        color: #4057A1;\n",
        "        font-style: italic;\n",
        "        margin-bottom: .5em;\n",
        "        margin-top: 0.5em;\n",
        "        display: block;\n",
        "    }\n",
        "    \n",
        "    .warning{\n",
        "        color: rgb( 240, 20, 20 )\n",
        "        }  \n",
        "</style>\n",
        "<script>\n",
        "    MathJax.Hub.Config({\n",
        "                        TeX: {\n",
        "                           extensions: [\"AMSmath.js\"]\n",
        "                           },\n",
        "                tex2jax: {\n",
        "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
        "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
        "                },\n",
        "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
        "                \"HTML-CSS\": {\n",
        "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
        "                }\n",
        "        });\n",
        "</script>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "<IPython.core.display.HTML at 0x7fc408826a50>"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}